"""Migration helper that loads GraphRAG artifacts into the Deep Insight ontology tables.

Usage
-----
python scripts/migrate_graphrag_to_postgres.py \
    --dsn postgresql://user:password@host:5432/cipgraph \
    --entities c:/data/graphrag/entities.parquet \
    --relationships c:/data/graphrag/relationships.parquet \
    --clauses c:/data/graphrag/clauses.parquet

Configuration
-------------
* DSN must point to the Azure Database for PostgreSQL flexible server with the ontology schema deployed.
* GraphRAG artifacts are expected to follow the default column layout produced by the `graphrag` pipeline.
  - entities.parquet: columns [id, type, name, description, properties]
  - relationships.parquet: columns [source_id, target_id, type, weight, properties]
  - clauses.parquet: columns [contract_id, section_label, title, text, embedding]
* The script is idempotent. It performs upserts keyed by the natural IDs generated by GraphRAG.

Dependencies
------------
* psycopg[binary,pool] >= 3.2
* pyarrow >= 14.0
* python-dotenv (optional) if you prefer to load the DSN from an .env file.
"""

from __future__ import annotations

import argparse
import json
import logging
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Iterable, List, Sequence

import pyarrow.parquet as pq
from psycopg import sql
from psycopg.rows import dict_row
from psycopg_pool import ConnectionPool

LOGGER = logging.getLogger("migrate.graphrag")


@dataclass(frozen=True)
class MigratorConfig:
    dsn: str
    entities_path: Path
    relationships_path: Path
    clauses_path: Path | None = None
    batch_size: int = 500


def _read_parquet(path: Path) -> List[dict[str, Any]]:
    if not path.exists():
        raise FileNotFoundError(path)
    table = pq.read_table(path)
    return table.to_pylist()


def _normalize_embedding(raw: Any) -> str | None:
    if raw is None:
        return None
    if isinstance(raw, str):
        try:
            values = json.loads(raw)
        except json.JSONDecodeError:
            LOGGER.warning("Embedding field is a non-JSON string; skipping")
            return None
    else:
        values = raw
    if not isinstance(values, Sequence):
        LOGGER.warning("Embedding payload is not a sequence; skipping")
        return None
    return "[" + ",".join(f"{float(v):.6f}" for v in values) + "]"


def _ensure_table(conn, ddl: str) -> None:
    with conn.cursor() as cur:
        cur.execute(ddl)


def _bulk_upsert(conn, insert_sql: str, rows: Sequence[Sequence[Any]]) -> None:
    if not rows:
        return
    with conn.cursor() as cur:
        cur.executemany(insert_sql, rows)


class GraphRAGMigrator:
    def __init__(self, config: MigratorConfig, pool: ConnectionPool) -> None:
        self.config = config
        self.pool = pool

    def run(self) -> None:
        LOGGER.info("Starting migration")
        entities = _read_parquet(self.config.entities_path)
        relationships = _read_parquet(self.config.relationships_path)
        clause_chunks = _read_parquet(self.config.clauses_path) if self.config.clauses_path else []

        LOGGER.info("Loaded %s entities, %s relationships, %s clause chunks", len(entities), len(relationships), len(clause_chunks))
        with self.pool.connection() as conn:
            conn.autocommit = False
            self._ensure_static_tables(conn)
            self._migrate_contracts(conn, entities)
            self._migrate_parties(conn, entities)
            self._migrate_clauses(conn, clause_chunks)
            self._migrate_relationships(conn, relationships)
            conn.commit()
        LOGGER.info("Migration completed successfully")

    # -------------------------------
    # Migration steps
    # -------------------------------
    def _ensure_static_tables(self, conn) -> None:
        LOGGER.debug("Ensuring ontology tables exist")
        _ensure_table(
            conn,
            """
            CREATE TABLE IF NOT EXISTS contracts (
                contract_id text PRIMARY KEY,
                external_ref text,
                contract_type text,
                effective_date timestamp with time zone,
                expiration_date timestamp with time zone,
                status text,
                governing_law text,
                source_markdown_location text,
                risk_score_overall numeric,
                created_at timestamptz DEFAULT now(),
                updated_at timestamptz DEFAULT now()
            );
            """,
        )
        _ensure_table(
            conn,
            """
            CREATE TABLE IF NOT EXISTS parties (
                party_id text PRIMARY KEY,
                name text,
                normalized_name text,
                is_internal boolean,
                jurisdiction_country text,
                jurisdiction_state text,
                risk_profile text,
                created_at timestamptz DEFAULT now(),
                updated_at timestamptz DEFAULT now()
            );
            """,
        )
        _ensure_table(
            conn,
            """
            CREATE TABLE IF NOT EXISTS clauses (
                clause_id text PRIMARY KEY,
                contract_id text REFERENCES contracts(contract_id),
                section_label text,
                title text,
                clause_type text,
                risk_level text,
                is_standard boolean,
                text text,
                embedding vector(1536),
                full_text_vector tsvector,
                created_at timestamptz DEFAULT now(),
                updated_at timestamptz DEFAULT now()
            );
            """,
        )
        _ensure_table(
            conn,
            """
            CREATE TABLE IF NOT EXISTS contract_party_edges (
                contract_id text REFERENCES contracts(contract_id),
                party_id text REFERENCES parties(party_id),
                role text,
                PRIMARY KEY (contract_id, party_id, role)
            );
            """,
        )
        _ensure_table(
            conn,
            """
            CREATE TABLE IF NOT EXISTS contract_clause_edges (
                contract_id text REFERENCES contracts(contract_id),
                clause_id text REFERENCES clauses(clause_id),
                PRIMARY KEY (contract_id, clause_id)
            );
            """,
        )

    def _migrate_contracts(self, conn, entities: Sequence[dict[str, Any]]) -> None:
        contract_rows: List[Sequence[Any]] = []
        for entity in entities:
            if entity.get("type", "").lower() != "contract":
                continue
            props = entity.get("properties", {}) or {}
            contract_rows.append(
                (
                    entity.get("id"),
                    props.get("source_id") or props.get("external_ref"),
                    props.get("contract_type"),
                    props.get("effective_date"),
                    props.get("expiration_date"),
                    props.get("status"),
                    props.get("governing_law"),
                    props.get("source_markdown_location"),
                    props.get("risk_score_overall"),
                )
            )
        LOGGER.info("Upserting %s contracts", len(contract_rows))
        _bulk_upsert(
            conn,
            """
            INSERT INTO contracts (
                contract_id, external_ref, contract_type, effective_date,
                expiration_date, status, governing_law, source_markdown_location,
                risk_score_overall
            ) VALUES (
                %s,%s,%s,%s,%s,%s,%s,%s,%s
            )
            ON CONFLICT (contract_id) DO UPDATE SET
                external_ref = EXCLUDED.external_ref,
                contract_type = EXCLUDED.contract_type,
                effective_date = EXCLUDED.effective_date,
                expiration_date = EXCLUDED.expiration_date,
                status = EXCLUDED.status,
                governing_law = EXCLUDED.governing_law,
                source_markdown_location = EXCLUDED.source_markdown_location,
                risk_score_overall = EXCLUDED.risk_score_overall,
                updated_at = now();
            """,
            contract_rows,
        )

    def _migrate_parties(self, conn, entities: Sequence[dict[str, Any]]) -> None:
        party_rows: List[Sequence[Any]] = []
        edge_rows: List[Sequence[Any]] = []
        for entity in entities:
            if entity.get("type", "").lower() != "party":
                continue
            props = entity.get("properties", {}) or {}
            party_rows.append(
                (
                    entity.get("id"),
                    entity.get("name") or props.get("name"),
                    props.get("normalized_name"),
                    props.get("is_internal"),
                    props.get("jurisdiction_country"),
                    props.get("jurisdiction_state"),
                    props.get("risk_profile"),
                )
            )
            for contract_id in props.get("contracts", []):
                edge_rows.append((contract_id, entity.get("id"), props.get("role")))
        LOGGER.info("Upserting %s parties", len(party_rows))
        _bulk_upsert(
            conn,
            """
            INSERT INTO parties (
                party_id, name, normalized_name, is_internal,
                jurisdiction_country, jurisdiction_state, risk_profile
            ) VALUES (%s,%s,%s,%s,%s,%s,%s)
            ON CONFLICT (party_id) DO UPDATE SET
                name = EXCLUDED.name,
                normalized_name = EXCLUDED.normalized_name,
                is_internal = EXCLUDED.is_internal,
                jurisdiction_country = EXCLUDED.jurisdiction_country,
                jurisdiction_state = EXCLUDED.jurisdiction_state,
                risk_profile = EXCLUDED.risk_profile,
                updated_at = now();
            """,
            party_rows,
        )
        LOGGER.info("Upserting %s contract/party edges", len(edge_rows))
        _bulk_upsert(
            conn,
            """
            INSERT INTO contract_party_edges (contract_id, party_id, role)
            VALUES (%s,%s,%s)
            ON CONFLICT (contract_id, party_id, role) DO NOTHING;
            """,
            edge_rows,
        )

    def _migrate_clauses(self, conn, clause_chunks: Sequence[dict[str, Any]]) -> None:
        clause_rows: List[Sequence[Any]] = []
        edge_rows: List[Sequence[Any]] = []
        for clause in clause_chunks:
            clause_id = clause.get("chunk_id") or clause.get("id")
            contract_id = clause.get("contract_id")
            embedding = _normalize_embedding(clause.get("embedding"))
            clause_rows.append(
                (
                    clause_id,
                    contract_id,
                    clause.get("section_label"),
                    clause.get("title"),
                    clause.get("clause_type"),
                    clause.get("risk_level"),
                    clause.get("is_standard"),
                    clause.get("text"),
                    embedding,
                    clause.get("text"),
                )
            )
            if contract_id:
                edge_rows.append((contract_id, clause_id))
        LOGGER.info("Upserting %s clauses", len(clause_rows))
        _bulk_upsert(
            conn,
            """
            INSERT INTO clauses (
                clause_id, contract_id, section_label, title, clause_type,
                risk_level, is_standard, text, embedding, full_text_vector
            ) VALUES (
                %s,%s,%s,%s,%s,%s,%s,%s,%s, to_tsvector('english', %s)
            )
            ON CONFLICT (clause_id) DO UPDATE SET
                section_label = EXCLUDED.section_label,
                title = EXCLUDED.title,
                clause_type = EXCLUDED.clause_type,
                risk_level = EXCLUDED.risk_level,
                is_standard = EXCLUDED.is_standard,
                text = EXCLUDED.text,
                embedding = EXCLUDED.embedding,
                full_text_vector = EXCLUDED.full_text_vector,
                updated_at = now();
            """,
            clause_rows,
        )
        LOGGER.info("Upserting %s contract/clause edges", len(edge_rows))
        _bulk_upsert(
            conn,
            """
            INSERT INTO contract_clause_edges (contract_id, clause_id)
            VALUES (%s,%s)
            ON CONFLICT (contract_id, clause_id) DO NOTHING;
            """,
            edge_rows,
        )

    def _migrate_relationships(self, conn, relationships: Sequence[dict[str, Any]]) -> None:
        # Placeholder to demonstrate how additional ontology edges are wired.
        # Extend this method to handle obligation/right/risk relationships as soon as those tables are available.
        LOGGER.info("Processed %s generic relationships (extend mappings as ontology tables become available)", len(relationships))


def _parse_args() -> MigratorConfig:
    parser = argparse.ArgumentParser(description="Migrate GraphRAG artifacts into PostgreSQL ontology tables")
    parser.add_argument("--dsn", default=os.getenv("CIP_POSTGRES_DSN"), help="PostgreSQL connection string")
    parser.add_argument("--entities", required=True, type=Path, help="Path to graph_entities.parquet")
    parser.add_argument("--relationships", required=True, type=Path, help="Path to graph_relationships.parquet")
    parser.add_argument("--clauses", type=Path, help="Path to clause_chunks.parquet (optional)")
    parser.add_argument("--batch-size", type=int, default=500)
    args = parser.parse_args()

    if not args.dsn:
        raise SystemExit("Provide a Postgres DSN via --dsn or CIP_POSTGRES_DSN environment variable")

    return MigratorConfig(
        dsn=args.dsn,
        entities_path=args.entities,
        relationships_path=args.relationships,
        clauses_path=args.clauses,
        batch_size=args.batch_size,
    )


def main() -> None:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(name)s - %(message)s")
    config = _parse_args()
    pool = ConnectionPool(conninfo=config.dsn, open=True, max_size=5)
    migrator = GraphRAGMigrator(config, pool)
    migrator.run()


if __name__ == "__main__":
    main()
